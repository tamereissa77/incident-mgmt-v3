import streamlit as st
import pandas as pd
import os
import google.generativeai as genai
import time
import glob
from streamlit.components.v1 import html

# --- Set up the dashboard page configuration ---
st.set_page_config(
    page_title="Tamer Incident Analysis",
    layout="wide",
    page_icon="ðŸš¨"
)

# --- Auto-Refresh Component ---
# This JavaScript will reload the page every 30 seconds.
st.html("""
    <script>
        setTimeout(function() {
            window.location.reload();
        }, 30000); // Time in milliseconds (30 seconds)
    </script>
""")

st.title("ðŸš¨ Tamer: Real-Time Incident Analysis Dashboard")

# --- Configure Gemini API ---
# It's crucial that the GEMINI_API_KEY is set in your docker-compose.yml
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-pro-latest')
else:
    st.error("âš ï¸ FATAL: GEMINI_API_KEY environment variable not found!")
    st.info("Please set the GEMINI_API_KEY in your docker-compose.yml file for the 'dashboard' service and restart the application.")
    st.stop()

# --- Data Loading Function ---
INCIDENT_DATA_PATH = "/opt/spark-data/data/enriched_incidents_hourly/"

# Cache the data for 60 seconds to improve performance and prevent re-reading on every interaction.
@st.cache_data(ttl=60)
def load_incident_data():
    """
    Load and process all incident data from the CSV files generated by Spark.
    """
    if not os.path.exists(INCIDENT_DATA_PATH):
        # Return an empty DataFrame with expected columns to prevent downstream errors
        return pd.DataFrame(columns=['event_time', 'level', 'message', 'incident_id', 'service', 'category'])
    
    csv_files = glob.glob(os.path.join(INCIDENT_DATA_PATH, "*.csv"))
    if not csv_files:
        return pd.DataFrame(columns=['event_time', 'level', 'message', 'incident_id', 'service', 'category'])
    
    dataframes = []
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            dataframes.append(df)
        except pd.errors.EmptyDataError:
            continue
        except Exception as e:
            st.warning(f"Could not read file {file}: {e}")
            continue

    if not dataframes:
        return pd.DataFrame(columns=['event_time', 'level', 'message', 'incident_id', 'service', 'category'])
        
    combined_df = pd.concat(dataframes, ignore_index=True)
    
    if 'event_time' in combined_df.columns:
        combined_df['event_time'] = pd.to_datetime(combined_df['event_time'])
        combined_df = combined_df.sort_values('event_time', ascending=False)
    
    return combined_df

# --- Advanced Prompt Creation Function ---
def create_correlation_analysis_prompt(primary_incident, correlated_incidents_df):
    """
    Creates an advanced prompt for Gemini, including correlated incidents.
    """
    correlated_incidents_str = ""
    if not correlated_incidents_df.empty:
        for index, row in correlated_incidents_df.iterrows():
            correlated_incidents_str += f"- **Time:** {row.get('event_time')} | **Level:** {row.get('level')} | **Message:** {row.get('message')}\n"
    else:
        correlated_incidents_str = "No other incidents found for this service in the time window."

    prompt = f"""
You are a top-tier Principal Site Reliability Engineer (SRE) and a world-class incident commander. 
Your task is to perform a deep, correlated root cause analysis based on a primary incident and a cluster of related events.

**PRIMARY INCIDENT FOR ANALYSIS:**
- **Incident ID:** `{primary_incident.get('incident_id', 'N/A')}`
- **Severity Level:** `{primary_incident.get('level', 'N/A')}`
- **Service Affected:** `{primary_incident.get('service', 'N/A')}`
- **Incident Category:** `{primary_incident.get('category', 'N/A')}`
- **Event Time:** `{primary_incident.get('event_time', 'N/A')}`
- **Raw Message:** `{primary_incident.get('message', 'N/A')}`

**CORRELATED INCIDENTS (for the same service in the same time window):**
{correlated_incidents_str}

**ANALYSIS REQUIRED (Format your response using Markdown):**

## ðŸ“ˆ Executive Summary
Provide a one-paragraph summary of the entire event, explaining what happened, the impact, and the resolution.

## ðŸ•µï¸ Correlated Root Cause Analysis
Based on the **entire cluster of incidents**, deduce the most likely **primary root cause**. Go beyond the single message. Explain the "why" behind the "what". Did a bad deployment cause a resource leak, leading to a critical failure? Did a network issue cause a cascade of errors?

##  timeline Of Events
Construct a plausible, minute-by-minute timeline of how the incident likely unfolded, using the timestamps from the correlated events. Start from the earliest event and end with the latest.

## ðŸ“‹ Recommended Action Plan
Provide a detailed, actionable checklist for short-term and long-term remediation.

### Short-term (To Fix Now)
- [ ] *Action 1 (e.g., "Roll back deployment XYZ on service '{primary_incident.get('service', 'N/A')}'").*
- [ ] *Action 2 (e.g., "Increase resource allocation for the '{primary_incident.get('service', 'N/A')}' containers").*

### Long-term (To Prevent Recurrence)
- [ ] *Action 1 (e.g., "Implement stricter pre-deployment checks for resource-intensive changes").*
- [ ] *Action 2 (e.g., "Improve alerting thresholds for CPU/Memory on the '{primary_incident.get('service', 'N/A')}' service").*
"""
    return prompt

# --- Main Application Logic ---

df = load_incident_data()

# --- Display KPI Metrics ---
st.subheader("Key Performance Indicators")
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("ðŸ“ˆ Total Incidents", len(df))
with col2:
    critical_count = len(df[df['level'] == 'ERROR']) if not df.empty and 'level' in df.columns else 0
    st.metric("ðŸš¨ Critical Incidents", critical_count)
with col3:
    warning_count = len(df[df['level'] == 'WARNING']) if not df.empty and 'level' in df.columns else 0
    st.metric("âš ï¸ Warning Level", warning_count)
with col4:
    unique_services = df['service'].nunique() if not df.empty and 'service' in df.columns else 0
    st.metric("ðŸ”§ Affected Services", unique_services)

st.markdown("---")

# --- Display Main Data Table and Filters ---
if not df.empty:
    st.subheader("Live Incident Feed")
    
    # Create a copy of the DataFrame to be filtered
    filtered_df = df.copy()

    # Create filter widgets
    level_options = ['All'] + filtered_df['level'].unique().tolist()
    service_options = ['All'] + filtered_df['service'].dropna().unique().tolist()
    
    filter_col1, filter_col2 = st.columns(2)
    with filter_col1:
        level_filter = st.selectbox("Filter by Level:", options=level_options)
    with filter_col2:
        service_filter = st.selectbox("Filter by Service:", options=service_options)

    # Apply filters to the DataFrame
    if level_filter != 'All':
        filtered_df = filtered_df[filtered_df['level'] == level_filter]
    if service_filter != 'All':
        filtered_df = filtered_df[filtered_df['service'] == service_filter]
    
    # Display the filtered data
    st.dataframe(filtered_df, use_container_width=True, height=300)
    
    # --- AI Analysis Section ---
    if not filtered_df.empty and 'incident_id' in filtered_df.columns and filtered_df['incident_id'].notna().any():
        st.markdown("---")
        st.subheader("ðŸ”¬ Select an Incident for Correlated AI Analysis")
        
        incident_options = filtered_df['incident_id'].dropna().unique().tolist()
        selected_incident_id = st.selectbox("Choose a primary incident to analyze:", options=incident_options)
        
        if selected_incident_id:
            # Get the primary incident's data
            primary_incident = filtered_df[filtered_df['incident_id'] == selected_incident_id].iloc[0]
            
            st.code(primary_incident.get('message', 'N/A'), language=None)
            
            # Find and display correlated incidents
            incident_time = pd.to_datetime(primary_incident['event_time'])
            incident_service = primary_incident['service']
            
            time_window = pd.Timedelta(minutes=30)
            start_time = incident_time - time_window
            end_time = incident_time + time_window

            correlated_df = df[
                (df['service'] == incident_service) &
                (df['event_time'] >= start_time) &
                (df['event_time'] <= end_time) &
                (df['incident_id'] != selected_incident_id)
            ].sort_values('event_time')

            if not correlated_df.empty:
                with st.expander(f"Found {len(correlated_df)} correlated incident(s) for service '{incident_service}'"):
                    st.dataframe(correlated_df)
            else:
                st.info(f"No other incidents found for service '{incident_service}' within a +/- 30 minute window.")

            if st.button("ðŸ¤– Perform Correlated Analysis with Gemini", type="primary"):
                with st.spinner("ðŸ¤– Gemini is correlating events and performing deep analysis..."):
                    try:
                        prompt = create_correlation_analysis_prompt(primary_incident.to_dict(), correlated_df)
                        response = model.generate_content(prompt)
                        
                        st.markdown("---")
                        st.subheader("ðŸ¤– AI-Powered Correlated Incident Analysis")
                        st.markdown(response.text)
                    except Exception as e:
                        st.error(f"âŒ An error occurred during AI analysis: {e}")
    else:
        st.info("No incidents match the current filter criteria.")
else:
    st.info("ðŸ“­ No incident data available yet. Waiting for data from the Spark pipeline...")
    if st.button('ðŸ”„ Refresh Data Manually'):
        st.cache_data.clear()
        st.rerun()

# --- Footer ---
st.markdown("---")
current_time = time.strftime("%Y-%m-%d %H:%M:%S")
st.caption(f"Page loaded at: {current_time}. Auto-refreshing every 30 seconds.")